version: "3.9"

services:
  object_tracking:
    user: root # Ja dees is fucked maar "Je KrIjGt EnKeL pUnTeN oP MaChInE lEaRnInG"
    build:
      context: ./yolo_container
      dockerfile: Dockerfile
    image: yolo_container
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    shm_size: 2gb
    volumes:
      # Mount project folder (editable)
      - ./yolo_container:/workspace
      # Mount host /dev/shm to container /dev/shm
      - /dev/shm:/dev/shm
    working_dir: /workspace
    command: python3 object_tracking.py
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

#  openpcdet:
#    build:
#      context: ./openpcdet
#      dockerfile: Dockerfile
#    image: openpcdet_image
#    environment:
#      - NVIDIA_VISIBLE_DEVICES=all
#      - CUDA_LAUNCH_BLOCKING=1
#      - PYTHONUNBUFFERED=1
#    working_dir: /workspace
#    volumes:
#      # Your local inference script
#      - ./openpcdet/run:/workspace/run
#      - ./openpcdet/checkpoints:/workspace/checkpoints
#      # Mount host /dev/shm to container /dev/shm
#      - /dev/shm:/dev/shm
#    command: python3 inference.py
#    stdin_open: true
#    tty: true
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: all
#              capabilities: [gpu]

# I found online that OpenPCDet is better for Lidar so trying that first
#  mmdet3d:
#    build:
#      context: ./mmdetection3d
#      dockerfile: Dockerfile
#    image: mmdet3d_light
#    environment:
#      - NVIDIA_VISIBLE_DEVICES=all
#      - CUDA_LAUNCH_BLOCKING=1
#      - PYTHONUNBUFFERED=1
#    working_dir: /workspace
#    volumes:
#      # Your local inference script
#      - ./inference.py:/workspace/inference.py
#      - ./checkpoints:/workspace/checkpoints
#      # Mount host /dev/shm to container /dev/shm
#      - /dev/shm:/dev/shm
#    command: python3 inference.py
#    stdin_open: true
#    tty: true
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: all
#              capabilities: [gpu]

# Pointpillars not working -> Model mismatch with GPU version so we would have to retrain from scratch
  pointpillars:
    build:
      context: ./pointpillars_lightweight
      dockerfile: Dockerfile
    image: pointpillars_light
    network_mode: host  # So it can connect with VM process on localhost
    working_dir: /workspace
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_LAUNCH_BLOCKING=1 # Helps debug cuda crashes if they happen
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/workspace/PointPillars

    # 3D Point Clouds are heavy on shared memory.
    # We mirror your existing setup but bump the size slightly to be safe.
    shm_size: 4gb
    volumes:
      - ./pointpillars_lightweight/run:/workspace/run
      - /dev/shm:/dev/shm
    command: python3 run/inference.py
    # for PyTorch 3D data processing to avoid "OS Error: Bus error"
    ulimits:
      memlock: -1
      stack: 67108864
    stdin_open: true
    tty: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
