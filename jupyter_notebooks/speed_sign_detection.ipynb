{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import pytesseract as ocr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay"
   ],
   "id": "6bebe8162d536c16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_set = {\n",
    "    \"traffic_sign.jpg\": \"90\",\n",
    "    \"traffic_sign_1.jpg\": \"30\",\n",
    "    \"traffic_sign_2.jpg\": \"30\",\n",
    "    \"traffic_sign_3.jpg\": \"60\",\n",
    "    \"traffic_sign_4.jpg\": \"60\",\n",
    "    \"traffic_sign_5.jpg\": \"30\",\n",
    "    \"traffic_sign_6.jpg\": \"30\",\n",
    "    \"traffic_sign_7.jpg\": \"90\",\n",
    "    \"traffic_sign_8.jpg\": \"90\",\n",
    "    \"traffic_sign_9.jpg\": \"90\",\n",
    "    \"traffic_sign_10.jpg\": \"90\",\n",
    "    \"traffic_sign_11.jpg\": \"90\",\n",
    "    \"traffic_sign_12.jpg\": \"90\",\n",
    "    \"traffic_sign_13.jpg\": \"90\",\n",
    "    \"traffic_sign_14.jpg\": \"90\",\n",
    "    \"traffic_sign_15.jpg\": \"90\",\n",
    "    \"traffic_sign_16.jpg\": \"30\",\n",
    "    \"traffic_sign_17.jpg\": \"30\",\n",
    "    \"traffic_sign_18.jpg\": \"30\",\n",
    "    \"traffic_sign_19.jpg\": \"30\",\n",
    "    \"traffic_sign_20.jpg\": \"30\",\n",
    "    \"traffic_sign_21.jpg\": \"30\",\n",
    "    \"traffic_sign_22.jpg\": \"30\",\n",
    "    \"traffic_sign_23.jpg\": \"30\",\n",
    "    \"traffic_sign_24.jpg\": \"30\",\n",
    "    \"traffic_sign_25.jpg\": \"60\",\n",
    "    \"traffic_sign_26.jpg\": \"60\",\n",
    "    \"traffic_sign_27.jpg\": \"60\",\n",
    "    \"traffic_sign_28.jpg\": \"60\",\n",
    "    \"traffic_sign_29.jpg\": \"60\",\n",
    "    \"traffic_sign_30.jpg\": \"60\",\n",
    "    \"traffic_sign_31.jpg\": \"60\",\n",
    "    \"traffic_sign_32.jpg\": \"60\",\n",
    "    \"traffic_sign_33.jpg\": \"60\",\n",
    "    \"traffic_sign_34.jpg\": \"60\",\n",
    "    \"traffic_sign_35.jpg\": \"stop\",\n",
    "    \"traffic_sign_36.jpg\": \"stop\",\n",
    "    \"traffic_sign_37.jpg\": \"stop\",\n",
    "    \"traffic_sign_38.jpg\": \"stop\",\n",
    "    \"traffic_sign_39.jpg\": \"stop\",\n",
    "    \"traffic_sign_40.jpg\": \"stop\",\n",
    "    \"traffic_sign_41.jpg\": \"stop\",\n",
    "    \"traffic_sign_42.jpg\": \"stop\",\n",
    "    \"traffic_sign_43.jpg\": \"stop\",\n",
    "    \"traffic_sign_44.jpg\": \"stop\",\n",
    "    \"traffic_sign_45.jpg\": \"stop\",\n",
    "    \"traffic_sign_46.jpg\": \"stop\",\n",
    "    \"traffic_sign_47.jpg\": \"stop\",\n",
    "    \"traffic_sign_48.jpg\": \"stop\",\n",
    "    \"traffic_sign_49.jpg\": \"stop\",\n",
    "    \"traffic_sign_50.jpg\": \"stop\",\n",
    "    \"traffic_sign_51.jpg\": \"stop\",\n",
    "    \"traffic_sign_52.jpg\": \"stop\",\n",
    "    \"traffic_sign_53.jpg\": \"stop\",\n",
    "    \"traffic_sign_54.jpg\": \"stop\",\n",
    "    \"traffic_sign_55.jpg\": \"stop\",\n",
    "    \"traffic_sign_56.jpg\": \"stop\",\n",
    "    \"traffic_sign_57.jpg\": \"stop\",\n",
    "    \"traffic_sign_58.jpg\": \"stop\",\n",
    "    \"traffic_sign_59.jpg\": \"stop\",\n",
    "    \"traffic_sign_60.jpg\": \"stop\",\n",
    "    \"traffic_sign_61.jpg\": \"stop\",\n",
    "    \"traffic_sign_62.jpg\": \"stop\",\n",
    "    \"traffic_sign_63.jpg\": \"stop\",\n",
    "    \"traffic_sign_64.jpg\": \"stop\",\n",
    "    \"traffic_sign_65.jpg\": \"stop\",\n",
    "    \"traffic_sign_66.jpg\": \"stop\",\n",
    "    \"traffic_sign_67.jpg\": \"stop\",\n",
    "    \"traffic_sign_68.jpg\": \"stop\",\n",
    "    \"traffic_sign_69.jpg\": \"stop\",\n",
    "    \"traffic_sign_70.jpg\": \"stop\",\n",
    "    \"traffic_sign_71.jpg\": \"stop\",\n",
    "    \"traffic_sign_72.jpg\": \"stop\",\n",
    "    \"traffic_sign_73.jpg\": \"stop\",\n",
    "    \"traffic_sign_74.jpg\": \"stop\",\n",
    "    \"traffic_sign_75.jpg\": \"30\",\n",
    "    \"traffic_sign_76.jpg\": \"30\",\n",
    "    \"traffic_sign_77.jpg\": \"30\",\n",
    "    \"traffic_sign_78.jpg\": \"30\",\n",
    "    \"traffic_sign_79.jpg\": \"30\",\n",
    "    \"traffic_sign_80.jpg\": \"30\",\n",
    "    \"traffic_sign_81.jpg\": \"30\",\n",
    "    \"traffic_sign_82.jpg\": \"30\",\n",
    "    \"traffic_sign_83.jpg\": \"30\",\n",
    "    \"traffic_sign_84.jpg\": \"30\",\n",
    "    \"traffic_sign_85.jpg\": \"30\",\n",
    "    \"traffic_sign_86.jpg\": \"30\",\n",
    "    \"traffic_sign_89.jpg\": \"30\",\n",
    "    \"traffic_sign_90.jpg\": \"30\",\n",
    "    \"traffic_sign_91.jpg\": \"30\",\n",
    "    \"traffic_sign_92.jpg\": \"30\",\n",
    "    \"traffic_sign_93.jpg\": \"30\",\n",
    "    \"traffic_sign_94.jpg\": \"30\",\n",
    "    \"traffic_sign_95.jpg\": \"stop\",\n",
    "    \"traffic_sign_96.jpg\": \"stop\",\n",
    "    \"traffic_sign_97.jpg\": \"stop\",\n",
    "    \"traffic_sign_98.jpg\": \"stop\",\n",
    "    \"traffic_sign_99.jpg\": \"stop\",\n",
    "    \"traffic_sign_100.jpg\": \"stop\",\n",
    "    \"traffic_sign_101.jpg\": \"stop\",\n",
    "    \"traffic_sign_102.jpg\": \"stop\",\n",
    "    \"traffic_sign_103.jpg\": \"stop\",\n",
    "    \"traffic_sign_104.jpg\": \"stop\",\n",
    "}"
   ],
   "id": "48c47310b1aa1cc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "img = cv2.imread(\"/home/s0203301/project/Data/traffic_signs/traffic_sign_104.jpg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "im_pil = Image.fromarray(img)\n",
    "\n",
    "plt.imshow(im_pil)\n",
    "plt.axis(\"off\")  # optional: hide axes\n",
    "plt.show()"
   ],
   "id": "449838866594a60f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Setup and Pre-processing\n",
    "# ---------------------------------------------------------\n",
    "# image = im_pil (Assuming im_pil is defined as in your snippet)\n",
    "width, height = im_pil.size\n",
    "image_raw = np.array(im_pil)\n",
    "\n",
    "# 2. Image Processing Steps (Saving intermediate states)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Step 1: Grayscale\n",
    "img_gray = cv2.cvtColor(image_raw, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Step 2: Contrast & Threshold\n",
    "# Note: We use the previous step (img_gray) as input\n",
    "img_contrast = cv2.convertScaleAbs(img_gray, alpha=1.5, beta=0)\n",
    "_, img_thresh = cv2.threshold(img_contrast, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# Step 3: Flood Fill\n",
    "h, w = img_thresh.shape\n",
    "edge_pixels = []\n",
    "edge_pixels += [(x, 0) for x in range(w)]          # Top row\n",
    "edge_pixels += [(x, h-1) for x in range(w)]        # Bottom row\n",
    "edge_pixels += [(0, y) for y in range(h)]          # Left column\n",
    "edge_pixels += [(w-1, y) for y in range(h)]        # Right column\n",
    "\n",
    "# 3a\n",
    "img_filled = img_thresh.copy()\n",
    "for x, y in edge_pixels:\n",
    "    mask = np.zeros((h + 2, w + 2), np.uint8)\n",
    "    cv2.floodFill(img_filled, mask, (x, y), 255)\n",
    "\n",
    "# 3b\n",
    "img_filled_two = img_filled.copy()\n",
    "for x, y in edge_pixels:\n",
    "    mask = np.zeros((h + 2, w + 2), np.uint8)\n",
    "    cv2.floodFill(img_filled_two, mask, (x, y), 0)\n",
    "\n",
    "# Step 4: Bitwise Not\n",
    "img_final = cv2.bitwise_not(img_filled_two)\n",
    "\n",
    "# 3. Plotting Side-by-Side\n",
    "# ---------------------------------------------------------\n",
    "# Create a figure with 1 row and 4 columns\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "# List of images and titles for easy iteration\n",
    "images = [img_gray, img_thresh, img_filled, img_filled_two, img_final]\n",
    "titles = [\"Grayscale\", \"Thresholded\", \"Flood Filled once\", \"Flood Filled twice\", \"Inverted (Final)\"]\n",
    "\n",
    "for ax, img, title in zip(axes, images, titles):\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "text: str = ocr.image_to_string(img_final, config=\"--psm 6\")\n",
    "stripped = text.strip().lower()\n",
    "print(stripped)"
   ],
   "id": "578e97e5f2d5e1ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import Optional\n",
    "\n",
    "def image_to_sign(image: np.ndarray) -> Optional[str]:\n",
    "    image_processed = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)                                        # Grayscale\n",
    "    image_processed = cv2.convertScaleAbs(image_processed, alpha=1.5, beta=0)                        # Increase contrast\n",
    "    _, image_processed = cv2.threshold(image_processed, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) # Binarize (threshold)\n",
    "\n",
    "    # Double flood Fill\n",
    "    h, w = image_processed.shape\n",
    "    corners = [(0,0), (0,h-1), (w-1,0), (w-1,h-1)]\n",
    "    for x, y in corners:\n",
    "        mask = np.zeros((h + 2, w + 2), np.uint8)\n",
    "        cv2.floodFill(image_processed, mask, (x, y), 255)\n",
    "    for x, y in corners:\n",
    "        mask = np.zeros((h + 2, w + 2), np.uint8)\n",
    "        cv2.floodFill(image_processed, mask, (x, y), 0)\n",
    "\n",
    "    # Config\n",
    "    # psm stands for \"page segmentation method\", the number behind is for the sub-option\n",
    "    #   0    Orientation and script detection (OSD) only.\n",
    "    #   1    Automatic page segmentation with OSD.\n",
    "    #   2    Automatic page segmentation, but no OSD, or OCR.\n",
    "    #   3    Fully automatic page segmentation, but no OSD. (Default)\n",
    "    #   4    Assume a single column of text of variable sizes.\n",
    "    #   5    Assume a single uniform block of vertically aligned text.\n",
    "    #   6    Assume a single uniform block of text.\n",
    "    #   7    Treat the image as a single text line.\n",
    "    #   8    Treat the image as a single word.\n",
    "    #   9    Treat the image as a single word in a circle.\n",
    "    #  10    Treat the image as a single character.\n",
    "    #  11    Sparse text. Find as much text as possible in no particular order.\n",
    "    #  12    Sparse text with OSD.\n",
    "    #  13    Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific.\n",
    "    text: str = ocr.image_to_string(image_cv, config=\"--psm 6\")\n",
    "    stripped = text.strip().lower()\n",
    "\n",
    "    # First exact mapping\n",
    "    exact_match = {\"90\", \"60\", \"30\", \"stop\"}\n",
    "    if stripped in exact_match:\n",
    "        return stripped\n",
    "\n",
    "    # Loose mapping\n",
    "    if \"9\" in text:\n",
    "        return \"90\"\n",
    "    elif \"6\" in text:\n",
    "        return \"60\"\n",
    "    elif \"3\" in text:\n",
    "        return \"30\"\n",
    "    if 'stop' in text:\n",
    "        return \"stop\"\n",
    "\n",
    "    return None"
   ],
   "id": "f46d86be7dd5c930"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Predict ---\n",
    "y_pred = []\n",
    "for filename in test_set.keys():\n",
    "    img = cv2.imread(f\"/home/s0203301/project/Data/traffic_signs/{filename}\")\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    im_pil = Image.fromarray(img)\n",
    "    image_cv = np.array(im_pil)\n",
    "    y_pred.append(image_to_sign(image_cv))\n",
    "\n",
    "# Replace None with \"unknown\"\n",
    "y_pred = [p if p is not None else \"unknown\" for p in y_pred]\n",
    "y_true = [t if t is not None else \"unknown\" for t in test_set.values()]\n",
    "\n",
    "# --- Compute confusion matrix ---\n",
    "labels = [\"30\", \"60\", \"90\", \"stop\", \"unknown\"]  # include all classes\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix for OCR Sign Classifier\")\n",
    "plt.show()\n",
    "\n",
    "# --- Classification report ---\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, labels=labels))"
   ],
   "id": "8f73b65c3e6ac61d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9d430a92711e37d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
