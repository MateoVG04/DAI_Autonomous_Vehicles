{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import random\n"
   ],
   "id": "2c9cd4a14f871830"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ],
   "id": "989b38ef22ae1aaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# state_dim: [distance_error, relative_velocity, host_velocity, acceleration] = 4\n",
    "\n",
    "# action_dim: the agent outputs only an acceleration_command = 1"
   ],
   "id": "f3d6bb864480b828"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)).to(device),\n",
    "            torch.FloatTensor(np.array(actions)).to(device),\n",
    "            torch.FloatTensor(np.array(rewards)).unsqueeze(1).to(device),\n",
    "            torch.FloatTensor(np.array(next_states)).to(device),\n",
    "            torch.FloatTensor(np.array(dones)).unsqueeze(1).to(device)\n",
    "        )\n"
   ],
   "id": "27b33ca1c7f50b82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T10:42:01.992830Z",
     "start_time": "2025-10-16T10:42:01.131719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.out = nn.Linear(256, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        action = self.max_action * torch.tanh(self.out(x))\n",
    "        return action\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "        return q_value\n"
   ],
   "id": "a9381c145955376b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# DDPG Agent\n",
    "# DDPG = continuous-action DQN\n",
    "class DDPG:\n",
    "    def __init__(self, state_dim, action_dim, max_action, lr_actor=1e-3, lr_critic=1e-3, gamma=0.99, tau=0.005, buffer_capacity=100000, batch_size=64):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.max_action = max_action\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Actor\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        # slowly updated copy for improving stability\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        # Critic\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        # slowly updated copy for improving stability\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        # optimizer\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        self.critic_criterion = nn.MSELoss()\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "    def select_action(self, state, noise_std=0.1):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        if self.actor.training:\n",
    "            # Add noise only during training\n",
    "            action = action + np.random.normal(0, noise_std, size=self.action_dim)\n",
    "        return np.clip(action, -self.max_action, self.max_action)\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer.buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        # Update Critic\n",
    "        with torch.no_grad():\n",
    "            next_action = self.actor_target(next_state)\n",
    "            target_q = self.critic_target(next_state, next_action)\n",
    "            target_q = reward + (1 - done) * self.gamma * target_q\n",
    "\n",
    "        current_q = self.critic(state, action)\n",
    "        critic_loss = self.critic_criterion(current_q, target_q)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Update Actor\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target networks\n",
    "        self.update_target_networks()\n",
    "\n",
    "    def update_target_networks(self):\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "        torch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.actor.load_state_dict(torch.load(filename + \"_actor\", map_location=device))\n",
    "        self.critic.load_state_dict(torch.load(filename + \"_critic\", map_location=device))\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n"
   ],
   "id": "3333ac24c2a258b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T09:17:44.942370Z",
     "start_time": "2025-10-23T09:17:44.885633Z"
    }
   },
   "cell_type": "code",
   "source": "from simulation.python_3_8_20_scripts.shared_memory_utils import CarlaWrapper",
   "id": "caa371ee0312a75c",
   "outputs": [],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
